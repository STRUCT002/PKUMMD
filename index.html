<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0050)http://www.icst.pku.edu.cn/course/icb/MRS_MCI.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>PKU-MMD</title>
<meta name="description" content="PKU-MMD">
<meta name="author" content="Chunhui Liu">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="PKU-MMD_files/project.css">

<script> 
	function coming_soon()
	{
		alert("We are cleaning up our code to make it more simple and readable");
	}
	</script><script type="text/x-mathjax-config">
	  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script><script type="text/javascript" src="PKU-MMD_files/MathJax.js">
	</script><script type="text/x-mathjax-config">
	    MathJax.Hub.Config({
	        TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
	        "HTML-CSS": {linebreaks: {automatic: true}},
	        SVG: {linebreaks: {automatic: true}}
	    });
	</script></head>

	 
	
	
	

<body>
<div id="main">
  
	<div class="content"><br>
		<div class="title">
			<p class="banner" align="center"></p>
			<h1>PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding</h1>
		</div>

		<div class="authors">
			<div class="author">
				 <a href="http://www.icst.pku.edu.cn/struct" style="text-decoration: none">Spatial and Temporal Resolution Up Conversion Team, ICST, Peking University</a>
				 <p>Sponsored by: CCF-Tencent Open Research Fund,</p>
				 <p>Microsoft Research Asia, project ID FY17-RES-THEME-013.</p>
		    </div>
		</div>
		 
		<br><hr>
		<div class="overview sec">
			<div class="picture_wrapper" width="50%">
		  		<img src="PKU-MMD_files/teaser.png" alt="Teaser" width="50%">
		  		<p style="text-align: left font-size:75%:">Fig.1 PKU 
Multi-Modality Dataset is a large-scale multi-modalities action 
detection dataset. This dataset contains 2 phases, phases #1 contains 51
 action categories, performed by 66 distinct subjects in 3 camera views.</p>
	  		</div>
	  	</div>

		<div class="abstract_sec">
			<h2>Abstract</h2>
			<div class="desp">
				<p style="text-align:justify">
				PKU-MMD is a new large scale benchmark for continuous multi-modality
 3D human action understanding and covers a wide range of complex human 
activities with well annotated information. PKU-MMD contains 2 phases, 
for action detection tasks with increasing difficulty. Phase 1 is 
large-margin action detection task. Phase 2 is small-margin action 
detection task. Our dataset also provides multi-modality data sources, 
including RGB, depth, Infrared Radiation and Skeleton. We believe this 
large-scale dataset will benefit future researches on action detection 
for the community.	
				 </p>
			</div>
		</div>
		<hr>
		<div class="download_sec">
			<h2>Resources</h2>
			<div>
				<li><strong>Paper</strong>: <a href="https://arxiv.org/abs/1703.07475"> arxiv </a></li>

				<li><strong>Data</strong>:<br>
				&nbsp;&nbsp;Phase #1: <a href="https://drive.google.com/drive/folders/0B20a4UzO-OyMUVpHaWdGMFY1VDQ?usp=sharing">GoogleDrive</a> <br>
				&nbsp;&nbsp;Phase #2: <a href="https://drive.google.com/drive/folders/1OYYXvH5ymvV0K0iA-eDrb-2aNJ6HAtpJ">GoogleDrive</a> <a href="https://pan.baidu.com/s/1X6GpJnlqGd1v4zeL8_Gjvg">BaiduYun</a> <br> 
				&nbsp;&nbsp;<b>P.S.</b> We highly recommend downloading the resources by GoogleDrive or BaiduYun client for efficiency.
				</li><li><strong>Code</strong>: <a href="https://github.com/ECHO960/PKU-MMD">Evaluation protocol</a></li>

			</div>
		</div>

		<div class="citation_sec">
			<h2>Citation</h2>
			<p class="bibtex">
@article{liu2017pku, 
  title={PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding},
  author={Chunhui, Liu and Yueyu, Hu and Yanghao, Li and Sijie, Song and Jiaying, Liu},
  journal={arXiv preprint arXiv:1703.07475},
  year={2017}
}
			</p>		
		</div>
		<hr>
		<div class="abstract_sec">
			<h2>Dataset Description</h2>
			<div class="desp">
				PKU-MMD is our new large-scale dataset focusing on long continuous 
sequences action detection and multi-modality action analysis. The 
dataset is captured via the Kinect v2 sensor. <br><br>

				<b>Phase #1</b> contains 1076 long video sequences in 51 action 
categories, performed by 66 subjects in three camera views. It contains 
almost 20,000 action instances and 5.4 million frames in total. Each 
video lasts about 3$\sim$4 minutes (recording ratio set to 30 FPS) and 
contains approximately 20 action instances. The total scale of our 
dataset is 5,312,580 frames of 3,000 minutes with 21,545 temporally 
localized actions. <br>

				We choose 51 action classes in total, which are divided into two 
parts: 41 daily actions (drinking, waving hand, putting on the glassed, <i>etc.</i>) and 10 interaction actions (hugging, shaking hands, <i>etc.</i>).
 66 distinct subjects are invited for our data collection. Each subjects
 takes part in 4 daily action videos and 2 interactive action videos.our
 videos only contain one part of the actions, either daily actions or 
interaction actions. We design 54 sequences and divide subjects into 9 
groups, and each groups randomly choose 6 sequences to perform. <br><br>

				<b>Phase #2</b> contains 1009 short video sequences in 41 action 
categories, performed by 13 subjects in three camera views.  Each video 
lasts about 1$\sim$2 minutes (recording ratio set to 30 FPS) and 
contains approximately 7 action instances.<br><br>

				We provide 5 categories of resources: depth maps, RGB images, skeleton joints, infrared sequences, and RGB videos. 
				<li>Depth maps are sequences of two dimensional depth values in millimeters. The resolution is $512\times424$.</li>
				<li>Joint information consists of 3-dimensional locations of 25 
major body joints for detected and tracked human bodies in the scene. We
 further provide the confidence of each joints point as appendix. </li>
				<li>RGB videos are recorded in the provided resolution of $1920\times1080$.</li>
				<li>Infrared sequences are also collected and stored frame by frame in $512\times424$.</li> 
			</div>
			<hr>
			<h2>Data Format</h2>
			<div class="format">
			<p> Video Files:<br> &nbsp;&nbsp;&nbsp;&nbsp;
			RGB files are compressed to avi videos in 30 FPS using ffmpeg. File 
name format is $XXXX-V.avi$ for No. $XXXX$ video file in view $V$. For 
example $0001-L.avi$ is the first video in left view.</p>
			<p> Skeleton Files: <br>&nbsp;&nbsp;&nbsp;&nbsp;
			For each video, there exists a skeleton file $XXXX-V.skeleton$ which 
contains several lines for frame-level skeleton data. Each line contains
 $3\times 25\times 2$ float numbers for 3-dimensional locations of 25 
major body joints of 2 subjects.
			</p>

			<p> Label Files:<br>&nbsp;&nbsp;&nbsp;&nbsp;
			For each video, there exists a label file named $XXXX-V.label$ 
illustrating the ground truth labels. Several lines will be given, each 
line contains 4 integers for $Label, start, end, confidence$ 
respectively. Note that $confidence$ is either $1$ or $2$ for slight and
 strong recommendation respectively.</p>
			
			<p> Depth Files:<br>&nbsp;&nbsp;&nbsp;&nbsp;
			 A folder is provided for each video which contains several images 
corresponding to each frame. Each image is in $two-dimensional$ 
$512\times 424$ png format.</p>

			<p> RGB Files:<br>&nbsp;&nbsp;&nbsp;&nbsp;
			 A folder is provided for each video which contains several images 
corresponding to each frame. Each image is in $three-dimensional$ 
$1920\times 1080$ jpeg format.</p>

			<p> Infrared Files:<br>&nbsp;&nbsp;&nbsp;&nbsp;
			 A folder is provided for each video which contains several images 
corresponding to each frame. Each image is in $one-dimensional$ 
$512\times 424$ png format.</p>
			
			</div>
			<hr>
			<h2>Metrics</h2>			
			<div class="desp">
				<p>For the detection task, there is a basic criterion to evaluate 
the overlapping ratio between the predicted action interval $I$ and the 
ground truth interval $I^{*}$ with a threshold $\theta$. The detection 
interval is correct when
				\begin{equation}
			\frac{|I \cap I^{*}|}{|I \cup I^{*}|} &gt; \theta.
			\end{equation}
			where $I \cap I^{*}$ denotes the intersection of the predicted and 
ground truth intervals and $I \cup I^{*}$ denotes their union. So, with 
$\theta$, the precision $p(\theta)$ and recall $r(\theta)$ can be 
calculated.
			</p>

			<li><b>F1-Score</b>: With the above criterion to determine a correction detection, the F1-score is defined as
\begin{equation}
\mbox{F1}(\theta)=2 \cdot \frac{p(\theta) \times r(\theta)}{p(\theta) + r(\theta)}.
\end{equation}</li>

			<li><b>Interpolated Average Precision (AP)</b>: With confidence 
changing, precision and recall values can be plotted to give a 
precision-recall curve. Note that $r$ is also determined by overlapping 
confidence $\theta$. The interpolated average precision is calculated by
 the arithmetic mean of the interpolated precision at each recall level.
			\begin{equation}
			\mbox{AP} = \int_{0}^{1} \max_{r' \ge r} p(r') \; \mathrm{d}r.
			\label{APFORM}
			\end{equation}
			</li>
			<br>
			<li><b>Mean Average Precision of Actions (mAP$_a$)</b>: AP is used as
 the metric for evaluating the results on each activity category. The AP
 is averaged over all the activity categories. 
			</li>
			<br>
			<li><b>Mean Average Precision of Videos (mAP$_v$)</b>: AP is used as the metric for evaluating the results on each video. The AP is averaged over all the videos. 
			</li><br>
			<b>P.S.</b> For more information on Interpolated Average Precision, please read: <a href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html">Evaluation of ranked retrieval results</a>.
			</div>
		</div>
		<hr>
		<div class="experiments_sec sec">
		<h2>More Samples</h2>

			<div class="picture_result">
				<img src="PKU-MMD_files/overview.png" alt="Teaser" width="100%">

		  	<p style="text-align: left font-size:75%:">Fig.2 From top to bottom, these four rows show RGB, depth, skeleton and IR modalities, respectively.</p>

				<img src="PKU-MMD_files/samples.png" alt="Teaser" width="100%">

		  	<p style="text-align: left font-size:75%:">Fig.3 We collect 51 actions performed by 66 subjects, including actions for single and pairs.</p>
			</div>


		</div>
		<hr>
		<h2>FAQs</h2>
		<div>
		<li><strong>How can I obtain the coordinates of human skeletal joints on the RGB frames ? </strong>:
			<br>
			We did not record the coordinates of skeletons in the RGB in our dataset. But they can be calculated with the camera paramter matrix $M$.
			<br>
			M = [ <br>
	         	[1.03e3,     0,                 9.80e2], <br>
	         	[0,             1.03e3,         5.50e2], <br>
	         	[0,                0,                 1     ]  <br>
	      		]
	        <br>
			For a skeleton coordinate $p = [x,y,z]^T$ in the provided data, we can get the corresponding coordinate $p' = [x',y']^T$ in the RGB image by,

			$[a,b,c]^T = Mp$ <br>
			$x' = a/c$ <br>
			$y' = b/c$ <br>
		</li>


		</div>	
		<hr>
		<h2>Baselines</h2>
		<div>
		Here we show our baseline models for action recognition.
		<li> <strong>LSTM </strong>:
			The merits of LSTM layers allow the network to exploit the history information and model the temporal dynamics efficiently. We adopt LSTM to achieve action recognition on each modal data. Our LSTM network is compose of 3 layers, as shown in Fig. 4(a).
		<li><strong>BLSTM</strong>:
			To better utilize both history and future information, we also exploit Bidirectional LSTM (BLSTM) to test action recognition with each modal data.  The structure of our BLSTM model can be viewed in Fig. 4(b).
		<br>
		<br>	
		*Note that for RGB/optical flow/depth/IR data, we extract deep features from convolutional layers and then feed them to the following recurrent layers. More specifically, each frame is finally represented by a vector of dimension 1024 from the layer global pool of BN-Inception network. For skeletons, we directly feed the data to the recurrent layers.
				<div class="picture_result">
					<img src="PKU-MMD_files/LSTM-model.png" alt="Teaser" width="75%">
			  	<p style="text-align: center font-size: 75%:">Fig.5 The structure of LSTM and BLSTM modesls for action recognition. </p>
				</div>
		The detailed configuration of LSTM/BLSTM models for each modal data are as follows:
				<div class="picture_result">
					<p style="text-align: center font-size:75%:">Table 1. The number of neurons in LSTM and BLSTM networks. </p>
					<img src="PKU-MMD_files/LSTM_configuration.png" alt="Teaser" width="75%">
				</div>
		And we show our baseline results below (R: RGB, F: optical flow, D: depth, IR: infrared, S: skeleton):
				<div class="picture_result">
					<p style="text-align: center font-size:75%:">Table 2. Action recognition baseline results on Part I and Part II, respectively. (Acc. %)</p>
					<img src="PKU-MMD_files/part1-baseline.png" alt="Teaser" width="100%">
					<img src="PKU-MMD_files/part2-baseline.png" alt="Teaser" width="100%">
				</div>
		</div>

		  	

<hr>
		

	<p class="banner" align="center">Last update: Aug. 2019</p>
	
  </div>
</div>


</body></html>
